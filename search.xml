<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Test</title>
    <url>/2020/05/18/Test/</url>
    <content><![CDATA[<p><span class="math display">\[
\begin{equation}\label{eq2}
\begin{aligned}
a &amp;= b + c \\
   &amp;= d + e + f + g \\
   &amp;= h + i
\end{aligned}
\end{equation}
\]</span> <a id="more"></a></p>
<h2 id="section">1</h2>
<p>In the EM algorithm for Bayesian Linear Regression: <span class="math display">\[
\bf{y} \iff X \\
\bf{w} \iff Z \\
\alpha, \beta \iff \boldsymbol\theta
\]</span> As for the notation, <span class="math inline">\(N\)</span> is the number of the data points, <span class="math inline">\(D\)</span> is the number of attributes.</p>
<p>First, we assume <span class="math inline">\(y=w^Tx+\epsilon\)</span>, where <span class="math inline">\(\epsilon \sim N(0,\frac{1}{\beta})\)</span>. If we rewrite the data as X each ro corresponds to a data point, then <span class="math display">\[
p(y|w,X,\beta)=N(y|Xw,\beta^{-1}I) \\
=\frac{1}{(2\pi)^{N/2}|\beta^{-1}|^{N/2}}\exp(-\frac{1}{2}\beta(y-Xw)^T(y-Xw))
\]</span> We define the prior of <span class="math inline">\(w\)</span> as <span class="math display">\[
\begin{equation}
\begin{aligned}
p(w|\alpha) &amp;=N(w|0,\alpha^{-1}I) \\
&amp;=\frac{1}{(2\pi)^{D/2}|\alpha^{-1}|^{D/2}}\exp(-\frac{1}{2}\alpha w^Tw)
\end{aligned}
\end{equation}
\]</span> Assuming <span class="math inline">\(w,X,\beta\)</span> are mutually independent, <span class="math display">\[
p(y,w|X,\alpha,\beta) = p(y|w,X,\beta)p(w|\alpha) \sim N(w|w_N, V_N) \\
p(w|X,y,\alpha,\beta) \propto N(w|w_N, V_N) \\
w_N = \beta V_NX^{T}y \\
V_N^{-1}=\alpha I+\beta X^{T}X
\]</span> For EM algorithm, we first initialize <span class="math inline">\(\alpha^{old}, \beta^{old}\)</span>.</p>
<p>E step: <span class="math display">\[
p(Z|X,\theta^{old})=p(y,w|X,\alpha^{old},\beta^{old}) \\
Q(\theta, \theta^{old})=E_{w|X,y,\alpha^{old},\beta^{old}}[\ln{p(y,w|X,\alpha, \beta)}]\\
=E[\ln{p(y|w,X,\beta)}] + E[\ln{p(w|\alpha)}] \\
= \frac{N}{2}\ln(\beta/2\pi)-\frac{\beta}{2}E[(y-Xw)^{T}(y-Xw)]+\frac{D}{2}\ln(\alpha/2\pi)-\frac{\alpha}{2}E(w^Tw)\\
\]</span> M step: <span class="math display">\[
\theta^{new} = \arg \max_{\theta} Q(\theta, \theta^{old})
\]</span> To generate <span class="math inline">\(\beta^{new}\)</span>, we can set <span class="math inline">\(\frac{\partial Q(\theta, \theta^{old})}{\partial \beta}=0\)</span> <span class="math display">\[
\begin{align}
\frac{\partial Q(\theta, \theta^{old})}{\partial \beta} =0 \\
\beta^{new} = \frac{N}{E_{w|X,y,\alpha^{old},\beta^{old}}[(y-Xw)^T(y-Xw)]}
\end{align}
\]</span> Let <span class="math inline">\(\boldsymbol{\epsilon}=\bf{y}-X\bf{w}\)</span>, where <span class="math inline">\(\boldsymbol{\epsilon}\sim N(\bf{y}-Xw_N, X^TXV_N)\)</span> <span class="math display">\[
E[(y-Xw)^T(y-Xw)] = E[\boldsymbol{\epsilon}^{T}\boldsymbol{\epsilon}] \\  \nonumber
=E[(\boldsymbol{\epsilon}-E(\boldsymbol{\epsilon}))^T(\boldsymbol{\epsilon}-E(\boldsymbol{\epsilon}))+E(\boldsymbol{\epsilon})^TE(\boldsymbol{\epsilon})+2(\boldsymbol{\epsilon}-E(\boldsymbol{\epsilon}))^TE(\boldsymbol{\epsilon})] \\
=E[(\boldsymbol{\epsilon}-E(\boldsymbol{\epsilon}))^T(\boldsymbol{\epsilon}-E(\boldsymbol{\epsilon}))]+E(\boldsymbol{\epsilon})^TE(\boldsymbol{\epsilon}) \\
=E[Trace((\boldsymbol{\epsilon}-E(\boldsymbol{\epsilon}))^T(\boldsymbol{\epsilon}-E(\boldsymbol{\epsilon})))]+E(\boldsymbol{\epsilon})^TE(\boldsymbol{\epsilon})  \\
=Trace[X^TXV_N]+(\bf{y}-Xw_N)^T(\bf{y}-Xw_N)
\]</span> Thus <span class="math display">\[
\beta^{new} = \frac{N}{Trace[X^TXV_N]+(\bf{y}-Xw_N)^T(\bf{y}-Xw_N)}
\]</span> Where <span class="math inline">\(w_N = \beta^{old} V_NX^{T}y\)</span> and <span class="math inline">\(V_N^{-1}=\alpha^{old} I+\beta^{old} X^{T}X\)</span>.</p>
<h2 id="section-1">2</h2>
<p>There are at most <span class="math inline">\(k^N\)</span> ways to partition <span class="math inline">\(N\)</span> data points into clusters, so there are finite possible assignments. For each iteration of the algorithm,</p>
<ol type="1">
<li>if the old assignment of centroids is same as the new, then the next clustering will keep same</li>
<li>if the new assignment has lower distortion measure than the old one, then the old assignment will be replaced by the new one.</li>
</ol>
<p>Since there is a finite number of possible assignments, each assignment has a unique minimum parameter of <span class="math inline">\(\boldsymbol{\mu}_k\)</span>, the K-meas algorithm will converge after a finite number of steps, when no re-assignment of data points will reduce the distortion measure. Also, if there are no different new assignment, <span class="math inline">\(\boldsymbol{\mu_k}\)</span> will keep unchanged.</p>
<h2 id="section-2">3</h2>
<p>Goal: <span class="math inline">\(\boldsymbol{\theta}_{MAP}=\arg \max_{\boldsymbol{\theta}}{p(\boldsymbol{\theta}|X)}=\arg \max_{\boldsymbol{\theta}}{p(X|\boldsymbol{\theta})p(\boldsymbol{\theta})}\)</span></p>
<ol type="1">
<li><p>Choose an initial setting for the parameters <span class="math inline">\(\boldsymbol{\theta}^{old}\)</span>, and assume a prior <span class="math inline">\(p(\boldsymbol{\theta})\)</span></p></li>
<li><p>E step:</p>
<p>Evaluate <span class="math inline">\(p(Z|X,\boldsymbol{\theta}^{old})\)</span> <span class="math display">\[
Q_{MAP}(\boldsymbol{\theta},\boldsymbol{\theta}^{old})=E_{Z|X,\boldsymbol{\theta}^{old}}[\log{p(X,Z|\boldsymbol{\theta})}+\log p(\boldsymbol{\theta})] \\
=E_{Z|X,\boldsymbol{\theta}^{old}}[\log{p(X,Z|\boldsymbol{\theta})]}+\log p(\boldsymbol{\theta})
\]</span></p></li>
<li><p>M step: <span class="math display">\[
\boldsymbol{\theta}^{new}=\arg \max_{\boldsymbol{\theta}}Q_{MAP}(\boldsymbol{\theta}, \boldsymbol{\theta}^{old})
\]</span></p></li>
<li><p>Check for the convergence of either the log posterior probability or the parameter values. If the convergence criterion is not satisfied, then let <span class="math display">\[
\boldsymbol{\theta}^{old}\leftarrow \boldsymbol{\theta}^{new}
\]</span> and return to step 2.</p></li>
</ol>
]]></content>
      <categories>
        <category>test</category>
      </categories>
      <tags>
        <tag>Test</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/02/17/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
</search>
